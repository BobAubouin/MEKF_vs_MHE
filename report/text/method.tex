In this section, different methods used to estimate the unknown parameters of the PD models are detailed. $BIS_0$ can be measured at before the induction of anesthesia and $E_{max}$ is usually set equal to $BIS_0$. Thus the remaining parameters are $C_{50p}$, $C_{50r}$, and $\gamma$. in this section, $\theta = \begin{pmatrix} C_{50p} & C_{50r} & \gamma \end{pmatrix}$ is used to describe the vector of unknown parameters.
\medskip

The Multi Extended Kalman Filter (MEKF) method select the best vector among a grid in space of the parameters. This discrete choice allow a fast convergence but less precision at the end. The Moving Horizon Estimation (MHE) method uses an extended state formulation to estimate the vector of parameters along with the state in a continuous manner. Thus the method could identify more precisely the parameters but is also more subject to noise and could be slower than MEKF.

\subsection{Multi Extended Kalman Filter}

In order to identify the PD parameters, the MEKF method uses a set of EKF, one for every realization of the vector selected within a grid in the space of the parameters. The grid is designed to reasonably represent the variability of the parameter vector. Next, a vector is chosen using a model-matching criterion. \medskip 

EKF is a state estimation method that relies on the linearization of a non-linear model. If we consider the model given in (\ref{eq:model}) with the non-linear function $f$ parametrized by $\theta$, the estimator using the parameter vector $\theta_i$ is given by:

\begin{flalign*}
&H_i(k) = \left. \frac{\partial f(x, \theta_i)}{\partial x} \right| _{x=\hat{x}_i(k_{|k-1})} \\
&K_i(k) = P_i(k_{|k-1})H_i^\top (k)(H_i(k)P_i(k_{|k-1})H_i^\top (k) + R_2)^{-1} \\
&\hat{x}_i(k_{|k}) = \hat{x}_i(k_{|k-1}) + K_i(k)(y(k) - f(\hat{x}_i(k_{|k-1}),\theta_i )) \\
&P_i(k_{|k}) = P_i(k_{|k-1}) - K_i(k) H_i(k) P_i(k_{|k-1}) \\
&\hat{x}_i(k+1_{|k}) =  A_d \hat{x}_i(k_{|k}) + B_d u(k) \\
&P_i(k+1_{|k}) = A_d P_i(k_{|k})A_d ^\top + R_1
\end{flalign*}

Here the notation $X(k_{1|k_2})$ represents the value of variable X computed at time step $k_1$ based on the knowledge available at $k_2$. The estimated state vector is $\hat{x}$ and $P$ is the covariance matrix. $A_d$ and $B_d$ are the matrix describing the discretized dynamic system (\ref{eq:model}) such that $x(k+1) = A_dx(k) + B_d u(k)$. $R_1$ and $R_2$ are two constant matrices used to respectively characterize the process uncertainties and the measurements noise.
\medskip


The idea is to select the "best" observer at each step time. To do so, two different criterions will be compared in this report. First the criterion proposed in \cite{petriImprovingEstimationPerformance2022}, called "Petri criterion" is tested. In this method, the estimation error on the output $e(k) = y(k) - f(x(k|_{k-1}), \theta_i)$ is used to construct a selection criterion for each observer. As in \cite{petriImprovingEstimationPerformance2022} the dynamic of the criterion for the $i^{th}$ observer is given by:

\begin{equation}
\eta_i (k+1) = \eta_i (k) + dt \left(- \nu \eta_i(k) + \lambda_1 |e(k)|^2 + \lambda_2 |K_i(k) e(k)|^2 \right),
\label{eq:criterion_dyn}
\end{equation}

where $\lambda_1, \lambda_2$, and $\nu$ are three positive design parameters and $dt$ the sampling time of the system. The criterion depends both on the output estimation error $e(k)$ and the correction effort of the observer $K_i(k) e(k)$. Equation (\ref{eq:criterion_dyn}) can be discretized with Euler method and the following equation can be deducted:

\begin{equation}
\eta_i(k) = e^{-\nu k dt} \eta_i(0) + \sum_{j=1}^{k} e^{-\nu(k-j)dt} (\lambda_1 |e(j)|^2 + \lambda_2 |K_i(j) e(j)|^2 ).
\end{equation} 

$\eta_i$ can be seen as a cost and the idea is to select the observer with the minimal cost at each step time. The index of the actually selected observer is denoted $i^*$.

\medskip


The second criterion is based on \cite{narendraAdaptiveControlUsing1997}, and will be called "Narendra criterion" in this report. The idea of this criterion is to perform an open loop simulation of the system started from the state estimation at time step $k - N$ and to compare the output of the simulation with the actual output. Here $N$ stand for the opent loop simulation windows size and is a tunning parameter. The criterion for the $i^{th}$ observer at time $k$ is given by: 

\begin{equation}
\eta_i(k) = \alpha (y(k) - f(\tilde{x}_i(k), \theta_i))^2 + \beta \sum^{N_c}_{l=0} e^{-\lambda l}(y(k-l) - f(\tilde{x}_i(k-l), \theta_i)^2
\end{equation}

$\tilde{x}_i(j)$ is the open loop state trajectory given by the following equation:

\begin{equation}
\begin{array}{ll}
\tilde{x}_i(j+1) &= A_d \tilde{x}_i(j) + B_d u(j) \quad \text{for} \quad j \in [k-N, k] \\
\tilde{x}_i(k-N) &= \hat{x}_i(k-N|_{k-N})
\end{array}
\end{equation}

This second solution is more computationally expensive than the first one because it requires to simulate all instance of the system $N$ times. However, it could be more precise as the criterion is directly related to the metric that we will use to compare the different methods. \medskip

Because those two solutions could produce too much switching between the observers, the parameter $\epsilon \in ]0,1]$ is introduced, and the switching is done at time step $k$ only if it exists $i\neq i^*$ such that $\eta_i(k)<\epsilon \eta_{i^*}$.
\medskip

To initialize the criterion of each observer, the distribution of the parameters can be taken into account. Particularly, considering the $\mathcal{C}_i$ the subset of the parameter space such that $\theta_i$ is the closer grid point to every point in $\mathcal{C}_i$ the criterion can be initialized by:

\begin{equation}
\eta_i(0) = \frac{\alpha}{p(\theta \in \mathcal{C}_i)},
\end{equation}

where $\alpha >0$ is a design parameter. \medskip

\subsection{Moving Horizon Estimation}

In order to include the estimation of $\theta$ along the state estimation, the MHE method uses an extended state formulation. The state vector is augmented with the parameter vector $\bar{x} = \begin{pmatrix} x \\ \theta \end{pmatrix}$ and the dynamic of the system is modified to include the dynamic of the parameters. The dynamic of the augmented state vector is given by:

\begin{equation}
\begin{array}{ll}
        \bar{x}(k+1)) = \begin{pmatrix}
        A & 0 \\
        0 & 0
        \end{pmatrix} \bar{x}(k) + \begin{pmatrix} B \\ 0 \end{pmatrix} u(k) = \Phi(\bar{x}(k), u(k))\\
        y(k) = f(\bar{x}(k)) + w(t).
\end{array}
\end{equation}

The optimization problem has been detailed in \cite{moussaDataBasedExtendedMoving2023} and is given by:

\begin{equation}
\begin{array}{ll}
\min_{\bar{\mathbf{x}}(k)} &  J_{N}(\bar{\mathbf{x}}(k), \hat{\bar{\mathbf{x}}}(k-1), \mathbf{y}^m,\mathbf{u}^m)   \\
 & \\
\text{s.t.} & \bar{\mathbf{x}}_k = \Phi(\hat{\bar{\mathbf{x}}}_{k-1},\mathbf{u}^m_{k-1}) \\
& \hat{\bar{\mathbf{x}}}_{k-1} = \bar{\mathbf{x}}_{k-1|k-1} \\

\end{array}
\end{equation}

Where the cost function is given by:

\begin{equation}
        J_{N}(\bar{\mathbf{x}}_k, \hat{\bar{\mathbf{x}}}_{k-1}, \mathbf{y}^m,\mathbf{u}^m) = \sum_{i=k-N}^{k}{\lVert y_i^m-h(\bar{x}_i) \rVert_Q} + \sum_{i=k-N+1}^{k}{\lVert \bar{x}_i-\Phi(\hat{\bar{x}}_{i-1},u^m_{i-1}) \rVert_{R(k)}}
\end{equation}

\normalsize
Where $\bar{\mathbf{x}}_k$, $\hat{\bar{\mathbf{x}}}_{k-1}$, $\mathbf{y}^m$, $\mathbf{u}^m$, $Q$ and $R$, respectively represent the state vector up to time $k$ and the previously estimated state up to time $k-1$ over the estimation horizon, the output and the input measurements over the estimation horizon and the penalty matrices.
\medskip

More detail on this method have been published in \cite{moussaDataBasedExtendedMoving2023}. \medskip



\subsection{Metrics for the comparison}

In order to compare the different methods, the true value of the parameters can not be compared to the estimated one. In fact, we also introduced uncertainties in the PK parameters in the simulation, and not in the estimators. Thus, the combination of the true parameters and the nominal PK parameters is not the same as the one used in the simulation. To overcome this problem, we stoped the estimation after different amount of time, performed an open loop simulation of the system with the estimated parameters and compared the output of the simulation with the actual output. The metric used to compare the different methods is the mean square error (MSE) between the actual output and the simulated one. The MSE is computed over the next two minutes after the end of the estimation. This time length could correspond to the MPC prediction horizon in a control framework for instance. Thus, the metrics computed at time step $k$ is given by:

\begin{equation}
    \text{MSE}(k) = \sum_{i=k}^{k+N_{MSE}} (y(i) - f(\hat{x}(i), \hat{\theta}(i)))^2,
\end{equation}

where $N_{MSE}$ is the number of step time in two minutes. \medskip

